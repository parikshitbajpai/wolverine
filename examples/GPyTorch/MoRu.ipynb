{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65dad5ae-52b7-41f0-a1a7-ecffedaa9b5f",
   "metadata": {},
   "source": [
    "# MoRu Phase Prediction using GPyTorch Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af84ce0-54ce-44b6-9651-289cfb145ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329b11a-7a9c-468e-8b36-6a4962438414",
   "metadata": {},
   "source": [
    "### Generate Dataset from JSON Dump\n",
    "\n",
    "The dataset $\\mathcal{D} \\subset \\mathbb{R}_{\\ge 0}^{n_\\text{samples} \\times (n_\\text{elements} + n_\\text{phases} + 2)}$ is defined as:\n",
    "$$\\mathcal{D} = \\{\\mathbf{x}_i \\, | \\, i \\in \\{1, \\dots, n_\\text{samples}\\}\\}$$\n",
    "where, $\\mathbf{x}_i$ is a single sample such that:\n",
    "$$\\mathbf{x}_i = \\left\\{{T, P, \\{c_j \\, | \\, j \\in \\mathcal{1, \\dots, n_\\text{elements}}\\}, \\{n_\\phi \\, | \\, \\phi \\in \\mathcal{1, \\dots, n_\\text{phases}}\\}}\\right\\}$$\n",
    "where, $c_j$ denotes number of moles of element $j$ and $n_\\phi$ denotes the number of moles of phase $\\phi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829187b-a3a7-4fc4-beb7-114505401631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = 'data/MoRu_sparse.json'\n",
    "stream = open(filename)\n",
    "data = json.load(stream)\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a08025-c069-4081-9724-2c3c2851faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = len(data)\n",
    "n_elements = len(data['2']['elements'])\n",
    "n_phases = len(data['2']['solution phases']) + len(data['2']['pure condensed phases'])\n",
    "dataset = np.zeros((n_data, n_elements + n_phases + 2))\n",
    "\n",
    "phase_names = list(data['2']['solution phases'].keys())\n",
    "phase_names += list(data['2']['pure condensed phases'].keys())\n",
    "element_names = list(data['2']['elements'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd07584-8d5f-4f55-815a-ed0897eae1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(data.keys())\n",
    "for i in keys:\n",
    "    dataset[keys.index(i), 0] = data[i][\"temperature\"]\n",
    "    dataset[keys.index(i), 1] = data[i][\"pressure\"]\n",
    "\n",
    "    for j in range(n_elements):\n",
    "        if not (data[i]['elements'].get(element_names[j]) is None):\n",
    "            dataset[keys.index(i), 2 + j] = data[i][\"elements\"][element_names[j]][\"moles\"]\n",
    "        else:\n",
    "            dataset[keys.index(i), 2 + j] = 0.0\n",
    "\n",
    "    for j in range(n_phases):\n",
    "        if not (data[i]['solution phases'].get(phase_names[j]) is None):\n",
    "            dataset[keys.index(i), 2 + n_elements + j] = data[i][\"solution phases\"][phase_names[j]][\"moles\"]\n",
    "        elif not (data[i]['pure condensed phases'].get(phase_names[j]) is None):\n",
    "            dataset[keys.index(i), 2 + n_elements + j] = data[i][\"pure condensed phases\"][phase_names[j]][\"moles\"]\n",
    "        else:\n",
    "            dataset[keys.index(i), 2 + n_elements + j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e16cc98",
   "metadata": {},
   "source": [
    "### Generate Training Data\n",
    "\n",
    "- Training samples $n_\\text{Train} = 0.6 * n_\\text{Total}$\n",
    "- Input $\\mathbf{X} = {T, P, \\mathbf{c}}$\n",
    "- Output $\\mathbf{y} = {\\mathbf{n}_\\phi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793731c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training = int(n_data * 0.6)\n",
    "\n",
    "choice = np.random.choice(range(dataset.shape[0]), size=(n_training,), replace=False)\n",
    "train_indices = np.zeros(dataset.shape[0], dtype=bool)\n",
    "train_indices[choice] = True\n",
    "test_indices = ~train_indices\n",
    "\n",
    "\n",
    "train = dataset[train_indices]\n",
    "test = dataset[test_indices]\n",
    "\n",
    "X_train = train[:,0:(2 + n_elements)]\n",
    "\n",
    "# Change the outputs to 0.0 / 1.0 based on the number of moles of phase\n",
    "y_train = (train[:,(2 + n_elements):] > 0.0).astype(float)\n",
    "\n",
    "# Use the original moles \n",
    "# y_train = train[:,(2 + n_elements):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_train_unscaled = X_train.detach().clone()\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_train_unscaled = y_train.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed28409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation of inputs\n",
    "X_mean = X_train.mean(0,keepdim=True)\n",
    "X_std = X_train.std(0,keepdim=True)\n",
    "\n",
    "# Standard deviation of pressure is equal to 0.0. Change this to 1.0 to avoid division by zero when scaling\n",
    "X_std[X_std == 0] = 1.0\n",
    "\n",
    "# Scale inputs to mean 0 and standard deviation 1 for better convergence\n",
    "X_train = (X_train - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c369ed",
   "metadata": {},
   "source": [
    "### Variational (Non-Gaussian Likelihood) Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025e6f3-a4e0-4a05-82fc-533bdf871c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import UnwhitenedVariationalStrategy\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "\n",
    "class GPClassificationModel(ApproximateGP):\n",
    "    def __init__(self, train_x):\n",
    "        variational_distribution = CholeskyVariationalDistribution(X_train.size(0))\n",
    "        variational_strategy = UnwhitenedVariationalStrategy(\n",
    "            self, X_train, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "\n",
    "# Initialize model and likelihood\n",
    "model = GPClassificationModel(X_train)\n",
    "likelihood = gpytorch.likelihoods.BernoulliLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89fa8b2-58fd-4704-a330-c3a90f30ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iterations = 2 if smoke_test else 5000\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# num_data refers to the number of training datapoints\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, y_train.numel())\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "for i in range(training_iterations):\n",
    "    # Zero backpropped gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Get predictive output\n",
    "    output = model(X_train)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, y_train)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27331f-fdf6-4660-a4dd-82fb9bb4c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test x are regularly spaced by 0.01 0,1 inclusive\n",
    "    test_x = torch.linspace(400, 2800, 101)\n",
    "    test_x = (test_x - test_x.mean(0)) / test_x.std(0)\n",
    "    # Get classification predictions\n",
    "    test_y = model(test_x)\n",
    "    observed_pred = likelihood(test_y)\n",
    "    upper, lower = test_y.confidence_region()\n",
    "\n",
    "    # Initialize fig and axes for plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Get the predicted labels (probabilites of belonging to the positive class)\n",
    "    # Transform these probabilities to be 0/1 labels\n",
    "    pred_labels = observed_pred.mean.ge(0.5).float()\n",
    "    ax.plot(test_x.numpy(), pred_labels.numpy(), 'b')\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    # ax.set_ylim([-0.1, 1.1])\n",
    "    ax.legend(['Observed Data', 'Mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc10d5d",
   "metadata": {},
   "source": [
    "### Exact GP Regression on Classification Label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa70be2-ba09-4b91-93d7-069d628ca46a",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298afc4-1582-4fcb-9a71-9b9deff812d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module =  gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bc2b0-2d92-4bf7-99e2-169ee34676ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6024d1-1de8-4158-9132-4aaf57fcd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 5000\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_ml = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -loss_ml(output, train_y)\n",
    "    loss.backward()\n",
    "    print(f'Iteration: {i + 1}/{training_iter} \\t Loss: {loss.item():.3f} \\t Lengthscale: {model.covar_module.base_kernel.lengthscale.item():.3f} \\t Noise:{model.likelihood.noise.item():.3f}')\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f49e8e-e25b-45f9-b346-e9dd9a2e3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(400, 2800, 101)\n",
    "    test_x = (test_x - test_x.mean(0)) / test_x.std(0)\n",
    "    observed_prediction = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4809a1c6-9764-4559-8e0d-ab26d1f4c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1)\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_prediction.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_prediction.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    # ax.set_ylim([-0.5, 2])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
