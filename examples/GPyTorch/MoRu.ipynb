{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65dad5ae-52b7-41f0-a1a7-ecffedaa9b5f",
   "metadata": {},
   "source": [
    "# MoRu Phase Prediction using GPyTorch Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af84ce0-54ce-44b6-9651-289cfb145ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329b11a-7a9c-468e-8b36-6a4962438414",
   "metadata": {},
   "source": [
    "##  Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829187b-a3a7-4fc4-beb7-114505401631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = 'data/MoRu.json'\n",
    "stream = open(filename)\n",
    "data = json.load(stream)\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a08025-c069-4081-9724-2c3c2851faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = np.zeros(len(data))\n",
    "pressure = np.zeros(len(data))\n",
    "mu_Mo = np.zeros(len(data))\n",
    "mu_Ru = np.zeros(len(data))\n",
    "Gibbs = np.zeros(len(data))\n",
    "BCCN = np.zeros(len(data))\n",
    "HCPN = np.zeros(len(data))\n",
    "Liquid = np.zeros(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd07584-8d5f-4f55-815a-ed0897eae1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(data.keys())\n",
    "for i in keys:\n",
    "    temperature[keys.index(i)] = data[i][\"temperature\"]\n",
    "    pressure[keys.index(i)] = data[i][\"pressure\"]\n",
    "    mu_Mo[keys.index(i)] = data[i][\"elements\"][\"Mo\"][\"element potential\"]\n",
    "    mu_Ru[keys.index(i)] = data[i][\"elements\"][\"Ru\"][\"element potential\"]\n",
    "    Gibbs[keys.index(i)] = data[i][\"integral Gibbs energy\"]\n",
    "    BCCN[keys.index(i)] = data[i][\"solution phases\"][\"BCCN\"][\"moles\"]\n",
    "    HCPN[keys.index(i)] = data[i][\"solution phases\"][\"HCPN\"][\"moles\"]\n",
    "    Liquid[keys.index(i)] = data[i][\"solution phases\"][\"LiqN\"][\"moles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ae903-3a40-49d2-8ce4-294512e23f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(temperature, mu_Mo)\n",
    "plt.plot(temperature, mu_Ru)\n",
    "plt.plot(temperature, Gibbs)\n",
    "plt.plot(temperature, BCCN)\n",
    "plt.plot(temperature, HCPN)\n",
    "plt.plot(temperature, Liquid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339905a-b0a5-45ac-af0c-f389678c879a",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b64fb-bf7a-4ab5-b050-d4641afa4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 50\n",
    "train_x = torch.linspace(0,1,n_train)\n",
    "train_y = torch.linspace(0,1,n_train)\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "indices = np.sort(rng.choice(len(data), size=n_train, replace=False))\n",
    "\n",
    "for i in range(n_train):\n",
    "    train_x[i] = temperature[indices[i]]\n",
    "    train_y[i] = (BCCN[indices[i]])\n",
    "\n",
    "plt.plot(train_x, train_y, '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025e6f3-a4e0-4a05-82fc-533bdf871c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import UnwhitenedVariationalStrategy\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "\n",
    "class GPClassificationModel(ApproximateGP):\n",
    "    def __init__(self, train_x):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = UnwhitenedVariationalStrategy(\n",
    "            self, train_x, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "\n",
    "# Initialize model and likelihood\n",
    "model = GPClassificationModel(train_x)\n",
    "likelihood = gpytorch.likelihoods.BernoulliLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89fa8b2-58fd-4704-a330-c3a90f30ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iterations = 2 if smoke_test else 100\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# num_data refers to the number of training datapoints\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, train_y.numel())\n",
    "\n",
    "for i in range(training_iterations):\n",
    "    # Zero backpropped gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Get predictive output\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27331f-fdf6-4660-a4dd-82fb9bb4c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test x are regularly spaced by 0.01 0,1 inclusive\n",
    "    test_x = torch.linspace(400, 2800, 101)\n",
    "    # Get classification predictions\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    print(observed_pred.mean.ge(0.1).float())\n",
    "\n",
    "    # Initialize fig and axes for plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Get the predicted labels (probabilites of belonging to the positive class)\n",
    "    # Transform these probabilities to be 0/1 labels\n",
    "    pred_labels = observed_pred.mean.ge(0.5).float()\n",
    "    ax.plot(test_x.numpy(), pred_labels.numpy(), 'b')\n",
    "    ax.set_ylim([-0.1, 1.1])\n",
    "    ax.legend(['Observed Data', 'Mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa70be2-ba09-4b91-93d7-069d628ca46a",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298afc4-1582-4fcb-9a71-9b9deff812d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module =  gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bc2b0-2d92-4bf7-99e2-169ee34676ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6024d1-1de8-4158-9132-4aaf57fcd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "loss_ml = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -loss_ml(output, train_y)\n",
    "    loss.backward()\n",
    "    print(f'Iteration: {i + 1}/{training_iter} \\t Loss: {loss.item():.3f} \\t Lengthscale: {model.covar_module.base_kernel.lengthscale.item():.3f} \\t Noise:{model.likelihood.noise.item():.3f}')\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f49e8e-e25b-45f9-b346-e9dd9a2e3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(400, 2800, 101)\n",
    "    observed_prediction = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4809a1c6-9764-4559-8e0d-ab26d1f4c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1)\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_prediction.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_prediction.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    # ax.set_ylim([-0.5, 2])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
